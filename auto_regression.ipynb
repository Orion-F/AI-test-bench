{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            sentence\n",
      "0  he was accorded a state funeral and was buried...\n",
      "1  in american english whilst is considered to be...\n",
      "2  once again she is seen performing on a compute...\n",
      "3     hippety hopper returns in mckimsons pop im pop\n",
      "4  today their programs are available on the inte...\n"
     ]
    }
   ],
   "source": [
    "# import some useful packages for making an auto regression ML model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df_sentences = pd.read_csv('cv-unique-no-end-punct-sentences.csv')\n",
    "\n",
    "# remove the first column (since it is just the index)\n",
    "df_sentences = df_sentences.drop(df_sentences.columns[0], axis=1)\n",
    "\n",
    "# make everything lowercase\n",
    "df_sentences = df_sentences.apply(lambda x: x.astype(str).str.lower())\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df_sentences.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.0425 - loss: 7.4331\n",
      "Epoch 2/20\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - accuracy: 0.0525 - loss: 6.6346\n",
      "Epoch 3/20\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - accuracy: 0.0571 - loss: 6.4302\n",
      "Epoch 4/20\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - accuracy: 0.0714 - loss: 6.2540\n",
      "Epoch 5/20\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - accuracy: 0.0867 - loss: 6.0089\n",
      "Epoch 6/20\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - accuracy: 0.0901 - loss: 5.8293\n",
      "Epoch 7/20\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - accuracy: 0.1085 - loss: 5.6252\n",
      "Epoch 8/20\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - accuracy: 0.1227 - loss: 5.4252\n",
      "Epoch 9/20\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - accuracy: 0.1340 - loss: 5.2483\n",
      "Epoch 10/20\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - accuracy: 0.1403 - loss: 5.0139\n",
      "Epoch 11/20\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - accuracy: 0.1514 - loss: 4.8316\n",
      "Epoch 12/20\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - accuracy: 0.1689 - loss: 4.6224\n",
      "Epoch 13/20\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - accuracy: 0.1816 - loss: 4.4877\n",
      "Epoch 14/20\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - accuracy: 0.2005 - loss: 4.2921\n",
      "Epoch 15/20\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - accuracy: 0.2154 - loss: 4.1254\n",
      "Epoch 16/20\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - accuracy: 0.2358 - loss: 4.0041\n",
      "Epoch 17/20\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - accuracy: 0.2574 - loss: 3.8350\n",
      "Epoch 18/20\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - accuracy: 0.2831 - loss: 3.7023\n",
      "Epoch 19/20\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - accuracy: 0.3121 - loss: 3.5109\n",
      "Epoch 20/20\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - accuracy: 0.3401 - loss: 3.3571\n",
      "Predicted next word: was\n"
     ]
    }
   ],
   "source": [
    "sentences = df_sentences.values.flatten()\n",
    "\n",
    "# use only the first 1000 sentences for training\n",
    "sentences = sentences[:1000]\n",
    "\n",
    "# Tokenize the sentences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Convert sentences into sequences of integers\n",
    "input_sequences = []\n",
    "for sentence in sentences:\n",
    "    token_list = tokenizer.texts_to_sequences([sentence])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "# Pad sequences to make them uniform length\n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')\n",
    "\n",
    "# Split predictors (X) and labels (y)\n",
    "X = input_sequences[:, :-1]\n",
    "y = input_sequences[:, -1]\n",
    "y = np.eye(total_words)[y]  # One-hot encode the labels\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 64, input_length=max_sequence_len-1))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, epochs=20, verbose=1)\n",
    "\n",
    "# Function to predict the next word\n",
    "def predict_next_word(model, tokenizer, text, max_sequence_len):\n",
    "    token_list = tokenizer.texts_to_sequences([text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    predicted = model.predict(token_list, verbose=0)\n",
    "    predicted_word = tokenizer.index_word[np.argmax(predicted)]\n",
    "    return predicted_word\n",
    "\n",
    "# Example usage\n",
    "seed_text = \"The quick brown fox\"\n",
    "next_word = predict_next_word(model, tokenizer, seed_text, max_sequence_len)\n",
    "print(\"Predicted next word:\", next_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_100_words(given_text):\n",
    "    for _ in range(100):\n",
    "        given_text += \" \" + predict_next_word(model, tokenizer, given_text, max_sequence_len)\n",
    "    return given_text\n",
    "\n",
    "test_text = predict_100_words(\"The quick brown fox\")\n",
    "\n",
    "# So this model is just a bigram model. Next steps could be to make also a 3-gram model, 4-gram model, etc.\n",
    "# and then combine them all together to make a more accurate model.\n",
    "# Then, start with 2-gram model to predict 2nd word, 3-gram model to predict 3rd word, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quick brown fox was all released on the snow and narrower a collision and hall on a collision on in the sand improvements watch on a national park of the sand mound buchan their campaigns in japan dakota on a race on in the sand mound poison their campaigns in a race railway team team of person on a car entered on a car user cluster their expansion user on a car entered on a street in a red shirt underneath a team and hall and alan roscoe on is a drug user on a pad in a red shirt and narrower on\n"
     ]
    }
   ],
   "source": [
    "print(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of all unique words in the by spliting all sentences into words and then getting the unique words\n",
    "unique_words = df_sentences['sentence'].str.split(expand=True).stack().unique()\n",
    "\n",
    "# sort the unique words\n",
    "unique_words.sort()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
